\chapter{Explainability}

\section{Causability and explainability of artificial intelligence in medicine}

	\subsection{Introduction}
	Recent research emphasizes more and more that AI systems should be able to build causal models of the world that support explanation and understanding.
	The grand challenges of ML are in sense-making, context understanding and decision making under uncertainty.
	The usable intelligence is difficult to reach because there is a need not only to learn from prior data, to extract knowledge, to generalize and to fight the curse of dimensionality, but to disentangle the underlying explanatory factors of the data in order to understand the context in an application domain.
	In medicine challenges of AI regards the integration, fusion and mapping of distributed and heterogeneous data up to the visual analysis of these.
	Explainable-AI in the context of medicine must take into account that diverse data may contribute to a relevant result.
	So there is a need for AI approaches that are performing well, trustworthy, transparent, interpretable and explainable for a human expert.
	Methods and models are necessary to reenact the machine decision making process, to reproduce and comprehend the learning and knowledge extraction.
	This is important because it is necessary to understand the causality of learned representation for decision support.
	The understanding of an AI model typically means a functional understanding of it, seeking to characterize the model's black-box behaviour without trying to elucidate its inner workings.
	There is a discrimination between interpretation (mapping of an abstract concept into a domain that the human expert can perceive) and explanation (a collection of feature of the interpretable domain that have contributed to a given example to produce a decision).

	\subsection{From explainability to causability}
	In the real world ground truth cannot always be well defined and human models are often based on causality as an ultimate aim for understanding underlying mechanisms with correlation as an intermediate step.
	When discussing the explainability of a machine statement there is a need to distinguish between:

	\begin{itemize}
		\item Explainability: in a technical sense highlights decision-relevant parts of the used representations of the algorithms and active parts in the algorithmic model that either contribute to its accuracy on the training set or to a specific observation.
			It does not refer to an explicit human model.
		\item Causability: as the extent to which an explanation of a statement to a human expert achieves a specified level of causal understanding with effectiveness, efficiency and satisfaction in a specified context of use.
	\end{itemize}

	Causability so refers to a human understandable model.
	There is need for a distinction between and explainable model and an explanation interface, which make the results gained in the explainable model usable and useful.
	Understanding is a bridge between perceiving and reasoning.
	Eplaining means to provide causes of observed phenomena in a comprehensible manner through a linguistic description of its logical and causal relationships.
	Causal explanations are the foundation of science in order to derive facts from laws and conditions in a deductive way.
	Directly understandable are data with dimension $\le\mathbb{R}^3$, that humans are able to perceive in a physiological sense with respect to their previous knowledge.

	\subsection{General approaches of explainable AI models}
	There are two types of explainable AI: posthoc explainability, occurring after the event in question and ante-hoc explainability, occurring before the event in question.


		\subsubsection{Posthoc systems}
		Posthoc systems aim to provide local explanations for a specific decision and make it reproducible on demand.
		An example si local interpretable model-agnostic explanation LIME, a model-agnostic system where $x\in\mathbb{R}^d$ is the original representation of an instance and $x'\in\mathbb{R}^{d'}$ is used to denote a vector for its interpretable repreesntation.
		The goal is to identify an interpretable model over the interpretable representation that is locally faithful to the classifier.
		The explanation model is $g:\mathbb{R}^{d'}\rightarrow\mathbb{R}$, with $g\in G$, and $G$ is a class of potentially interpretable models.
		$g$ can be visualized as an explanation to the human expert.
		Another example is black box explanations through transparent approximations BETA it explains the behaviour of any black-box classifier by simultaneously optimizing for fidelity to the original model and interpretability of the explanation.

		\subsubsection{Ante-hoc systems}
		Ante-hoc systems are interpretable by design towards glass-box approaches.
		They are linear regression, decision trees and fuzzy inference systems.
		An example are high performance generalized additive models with peirwise interactions or a visual explanations of the decisions of any classifier formulated as an additive model.

		\subsubsection{Interpreting a deep neural network}
		Typically NN are trained using supervised learning.
		Several approaches to probe and interpret DNN exists.
		Uncertainty provides a mearsure of how small perturbations of training data would change model parameters (model or epistemic uncertainty), or how input parameters changes would affect the prediction for one example (predictive or aleatoric variability).
		Model parameters can be approximated through variational methods, resulting in uncertainty information of model weights and a means to derive predictive uncertainty.
		This facilitates the appropriate use of model predictions.
		Aleatoric uncertainty can be differentiated into homoscedatic uncertainty (independent of an input) and heteroscedatic uncertainty (changing with different inputs to the system.
		Attribution seek to link a particular output of DAN to input variables.
		The gradients of the output when changing individual input variables can be analysed, tracing the prediction uncertainty to the componenent of a multivariate input.
		Activation maps can be used to identify parts of images relevant for a network prediction.
		Activation maximization identifies input patterns that lead to maximal activations relating to specific classes in the output layer, making the visualization of prototypes classes possible and assessing which properties the model capture for classes.
		This approach identifies highly probabile regions in the input space that create high output probabilities for a particular class.
		These can be found by a data density model in the standard objective function $\log p(w_c |x) -\lambda||x||^2$ that is maximized during training.
		The prototype is encouraged to produce strong class response and to resemble the data.
		The newly defined objective can be identified as the class conditioned data density $p(x|w_c)$.
		This corresponds to the most likely input $x$ for class $w_c$.
		A possible choice for this is the Gaussian restricted Boltzmann machine RBM, a two-layer bipartite undirected graphical model with a set of binary hidden unite $p(h)$, a set of visible units $p(v)$ with symmetric connection between the two layers represented by a weight matrix $W$.
		Its probability function is:

		$$\log p(x) = \sum\limits_jf_j(x) - \frac{1}{2}x^T\Sigma^{-1}x + cst\qquad f_j(x) = \log(1 + e^{w_j^Tx+b_j})$$

		In the case of model validation overfitting must be avoided as it could hide interesting failure modes of the model.
		Slightly underfitting can be enough.
		In the case of AM underfitting should be prevented because it could expose ptima $p(w_c|x)$ potentially distance from the data.

			\paragraph{Unsupervised learning and generative models}
			In certain application it is useful to provide a parametric representation of its density $p(x)$ or be able to sample from this density generating examples of the same type as the training examples.
			Boltzmann machines, autoencoders and GA are able to sample from this distribution:

			\begin{enumerate}
				\item Sample from a simple distribution $q(z)\sim N(0, I)$ defined in an abstract code space $Z$.
				\item Apply to the sample a decoding function $g:Z\rightarrow X$ that maps it back to the original input domain.
			\end{enumerate}

			The latent representation learned in these models can reflect complex relationship patterns in the data.
			Also generating instances provides means to study the difference of examples to a class.
			GANs can be incorporated in the maximization framework.
			The optimization problem is:

			$$\max\limits_{z\in Z}\log p(w_x|g(z))-\lambda||z||^2$$

			Where the first term is a composition of the newly introduced decoder and the original classifier.
			Once a solution $z^*$ to the optimization problem is found the prototype for $w_c$ is $x^* - g*(z^*)$.

	\subsection{Future outlook}

		\subsubsection{Weakly supervised learning}
		Weakly supervised learning describes methods to construct predictive models by learning with weak supervision: incomplete, inexact or inaccureate.
		Whole slide images can be classified according to widely  used scoring systems based on association with histomophological characteristics and an overall predictive score ad provide a relevance map generated observing the human expert during diagnosis making.

		\subsubsection{Structural causal models}
		Current AI work on a statistical or model-free mode.
		This entails sever limits on effectiveness and performance.
		They cannot reason about interventions and retrospection.
		To achieve human level intelligence AI need the guidance of a model of reality.
		To do so they propose a new visualization technique that can be trained by medical experts, as they can explore the underlying explanatory factors of the data and formalize a structural causal model of human decision making making and mapping feature in these to DL approaches.

		\subsubsection{Develop causability as a new scientific field}
		There is a need to develop causability methodologies based on clear scientific principles and theories of causality in order to establish causability as a scientific field
		Ca usability measures must ensure the quality of explanation.
		According to the three layer causal hierarchy:

		\begin{itemize}
			\item[Level 1]: Association $P(y|x)$ with the typical activity of seeing.
			\item[Level 2]: Intervention $P(y|do(x), z)$ with the typical activity of doing.
			\item[Level 3]: counterfactuals $P(y_x|x', y')$ with the typical activity of retrospections.
		\end{itemize}

		For each of these levels there is a need to develop methods to measure effectiveness, efficiency and user satisfaction.

\section{Explainable artificial intelligence: an analytical review}

	\subsection{Introduction}
	Deep learning is often characterized as a black box and opaque approach because of its large number of weights.
	On top of that their link to the physical environment is hard to isolate.
	This is problematic in highly sensitive areas.
	Because of its evergrowing importance the issues of transparency and explainability are increasingly critical.
	This is because this type of model can easily fool users and lead to dangerous or fatal consequences.
	The AI systems of the future will have to emulate the capabilities of humans to learn from very few examples and understand the concept behind the classification decision and articulate it to others.

	\subsection{XAI taxonomi}

	\begin{itemize}
		\item Transparency: a model is considered to be transparent if it has the potential to be understandable by itself.
		\item Interpretability: is defined as the capacity to provide interpretations in terms that are understandable to a human.
		\item Explainability: is related with the notion of exmplanation as an interface between humans and IA systems.
	\end{itemize}

	The ontology and taxonomy of XAI at a high level can be detailed:

	\begin{itemize}
		\item Transparent models: the decisions from these models are transparent.
		\item Opaque model: they are not transparent even though they obtain high transparency.
		\item Model agnostic: they are generally applicable and they have to be flexible enough so that they do not depend on the intrinsic architecture of the model and  they operate solely on the basis of relating the input to its outputs.
		\item Model specific: often take advantage of knowing a specific model and aim to bring transparency to a particular type of one or several models.
		\item Explanation by simplification: in these way alternatives to the original can be found to explain the prediction we are interested in.
			A linear model or a decision tree can be built around the predictions of a model, using the resulting model as a surrogate to explain the more complex one.
		\item Explanation by feature relevance: they attempt to evaluate a feature based on its average expected marginal contribution to the model's decision after all possible combinations have been considered.
		\item Visual explanation: the family of data visualization approaches can be exploited to interpret the prediction or decision over the input data.
		\item Local explanation: they approximate the model in a narrow area, around a specific instance of interest and offer information about how the model operates when encountering inputs that are similar to the one interested in explaining.
	\end{itemize}

	XAI taxonomy is divided in scope (local or global), usage (post hoc, surrogate models and intrinsic to the model architecture) and methodology (focused on the features or on the model parameters).
	The following fundamental principles which an AI must honour to be considered a XAI are:

	\begin{itemize}
		\item Explanation: an AI system must supply evidence, support or reasoning for each decision made by the system.
		\item Meaningful: the explanation provided by the system must be understandable by its user.
			This must be fine-tuned to meet the various characteristics of each group of users.
		\item Accuracy: the explanation provided by the system must reflect accurately the system's processes.
		\item Knowledge limits: systems must identify cases that they were not designed to operate in and where their answer may not be reliable.
	\end{itemize}

	\subsection{State of the art}

		\subsubsection{Feature oriented methods}
		Shapley additive explanation (SHAP) is a game-theoretic approach to explain ML predictions.
		It seeks to deduce the amount each feature contributed to the decision by representing them as players in a coalition game.
		The pay-off is an additive measure of importance, the weighted average contribution of a particular feature within every possible combination of feature.
		Local and global interpretations are consistent and the average prediction is distributed across all Shapley values, meaning that contrasting comparisons between explanations are possible.
		If the model is non additive then the interpretation of the values is not always transparent.

			\paragraph{Class activation maps}
			They are specific to CNNs, the per-class weighted linear sum of visual patterns present at various spatial locations.
			Global average pooling is applied to the final convulational feature map, which are then used as input features to a fully connected layer and output through a loss function.
			Projecting these wights of the output back to the previous convolutional layer, the areas in the input image with grater influence are highlighted per-class.
			Gradient-CAM generalize CAM to any arbitrary CNN architecture.
			The gradients for any target class are fed into the final convulational layer and an importance score computed in respect to the gradients.

		\subsubsection{Global methods}
		Global attribution mappings GAMs can explain NN's predictions on a global level for features with precise semantic definitions by formulating attributions as weighted conjoined rankings.
		Different subpopulation can be captured though a tunable granularity parmeter.
		They find a peri-wise rank distance matrix between features and a K-medoids clustering algorithm groups local feature importances into clusters.
		The medoid summarizes the pattern detected in each cluster as a global attribution.

			\paragraph{Gradient-based saliency maps}
			They are a visualization technique which render the absoulute value of the gradient of the majority predicted class as a normlized heatmap.
			The user can look at what features in the image are being used in the classification decision.

			\paragraph{Deep attribute maps}
			They are a technique for rendering the explainability of gradient-based methods.
			They evaluate between different saliencey-based explanation models.
			The gradient of the output is multiplied by the respective input to generate an explanation of a model's prediction in the form of a heatmap.

		\subsubsection{Concept models}
		Concept attivation vectors is a technique to explain globally the internal states of a NN by mapping human understandable features extracted by the NN.
		They represent the degree to which these abstract features point towards a set of human understandable concepts chosen by a user.
		Explaining the associated concept make possible to determine any defects in the decision-making process the model has learned.
		Automatic concept based explanations extract CAVs automatically without human supervision removing human bias.
		These concept are segmented at various spatial resolutions from in-class images.
		They are reliant on the concepts being uniquely meaningful to the class and the effectiveness of explanation is adversely affected if a chosen concept is commonly present in multiple classes.

		\subsubsection{Surrogate models}
		Local interpretable model-agnostic explanations LIME is a model-agnostic technique to create locally optimized explanations of ML models.
		LIME trains an interpretable surrogate model to learn the local behaviour of a global black-box model's prediction.
		An input image is divided into patches of contiguous superpixels and a weighted local model is trained on a new set of permuted instances of the oridingal image.
		By changing aspects of the input data human understandable and learning the differences between those perturbations and the original one can learn what about the input contributed to each class score.

		\subsubsection{Local, pixel-based methods}
		Layer-wise relevance propagation LRP uses predefined propagation rules to provide an explanation of a multilayered neural network's output in respect ot the input.
		It renders a heatmap, providing insight into which pixels contributed to the model's prediction and the extent to which they did.
		It hilights positive contributions to a network's decision.
		This process is posthoc and provides only a simplivide distillation of the features' role in the decision.

		\subsubsection{Human-centric methods}
		A cardinally different approach to explainability was proposed, which treats it as a human-centric pheonomena.
		People use similarity to associate new data with previously learned and aggregated prototypes, while statistic is based on averages.
