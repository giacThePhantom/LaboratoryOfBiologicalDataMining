\chapter{Autoencoders}

\section{Introduction}
An autoencoder is a specific type of a neural network, which is mainly designed to encode the input into a compressed an meaningful representation, and then decode it back such that the reconstructed input is as similar as possible to the original one.
Their main purpose is learning in an unsupervised manner an informative representation of the data that can be used for various implications.
The problem is to learn the functions encoder $A:\mathbb{R}^n\rightarrow\mathbb{R}^p$ and decoder $B:\mathbb{R}^p\rightarrow \mathbb{R}^n$ such that:

$$\arg\min\limits_{A, B}\mathbb{E}[\Delta(\vec{x}, B\circ A(\vec{X}))]$$

Where $\mathbb{E}$ is the expectation over the distribution of $\vec{x}$ and $\Delta$ is the reconstruction loss function, which measures the distance between the output of the decoder and the input, typically the $l_2$ norm.
Typically $A$ and $B$ are neural networks or linear operations (linear autoencoder).
An autoencoder is a generalization of PCA, where instead of finding a low dimensional hyperplane where the data lies, it is able to learn a non-linear manifold.
Autoencoders can be trained end-to-end or layer-by-layer.
In the latter case they are stacked together to a deep encoder.

	\subsection{Regularized autoencoders}
	Regularization is required since one may get the identity operator for $A$ and $B$ if it is not used.
	The most common option is to use a bottleneck, making the dimension of the representation smaller than the input.
	This directly creates a low dimensional representation of the data.
	It must still be possible to have overfitting if the capacity of the encoder and the decoder is large enough to encode each sample to an index.
	There are different possibility other than introducing a bottleneck.
	An important trade-off in autoencoders is the bias-variance trade-off: there is a need for an architecture able to reconstruct the input well.
	Such low-dimension representation should generalize to a meaningful one.

		\subsubsection{Sparse autencoders}
		To deal with this trade-off one can enforce sparsity on the hidden activations.
		This can be added on top of the bottleneck.
		To enforce sparsity regularization one can use ordinary regularization applied to the activations instead of the weights.

			\paragraph{$L_1$ regularization}
			$L_1$ regularization introduce sparsity, so the autoencoder optimization objective becomes:

			$$\arg\min\limits_{A,B}\mathbb{E}[\Delta(\vec{x}, B\circ A(\vec{x})] + \lambda\sum\limits_i|a_i|$$

			Where $a_i$ is the activation at the $i$th hidden layer and $i$ iterates over all of them.

			\paragraph{KL-divergence}
			Another way to enforce sparsity is to use the KL-divergence, a measure of the distance between two probability distributions.
			Instead of tweaking $\lambda$, one can assume that the activation of each neuron acts as a Bernoulli variable with probability $\hat{p}_j = \frac{1}{m}\sum\limits_ia_i(x)$, where $i$ iterates over the samples in the batch.
			The overall loss function would be:

			$$\arg\min\limits_{A,B}\mathbb{E}[\Delta(\vec{x}, B\circ A(\vec{x})] + \sum\limits_jKL(p||\hat{p}_j)$$

			Where the regularization term aims at matching $p$ to $\hat{p}$.

		\subsubsection{Denoising autoencoders}
		Denoising autoencoders can be considered as a regularization option or as robust autoencoders which can be used for error correction.
		The input is distributed by some noise and the autoencoder is expected to recunstruct the clean version of the input.
		Common options for $C$, the function that introduces with $\bar{x}$ a random variable are:

		$$C_\sigma(\bar{x}|\vec{x}) = N(\vec{x}, \sigma^2 I)$$

		Where $\sigma$ sets the impact of the noise.

		$$C_p(\bar{x}|\vec{x}) = \beta\odot\vec{x}, \beta\sim Ber(p)$$

		Where $\odot$ is an element-wise (Hadamard) product.
		Also $p$ sets the probability of a value in $\vec{x}$ not being nullified.

		\subsubsection{Contractive autoencoders}
		In contractive autoencoders the objective is to make the feature extraction less sensitive to small perturbations forcing the encoder to disregard changes in the input that are not important for the reconstruction by the decoder.
		To do so a penalty is imposed on the Jacobian of the network, trying to minimize its $L_2$ norm:

		$$\arg\min\limits_{A,B}\mathbb{E}[\Delta(\vec{x}, B\circ A(\vec{x})] + \lambda||J_A(\vec{x}||_2^2$$

		The reconstruction loss function and the regularization loss pull the result towards opposite directions.
		By minimizing the squared Jacobian norm, all the latent representation of the input tend to be more similar to each other making the reconstruction more difficult.
		The idea is that those variations that are not important for the reconstruction would be diminished by the regularization factor, while important variations would remain because of their impact on the reconstruction error.

	\subsection{Variational autoencoders}
	Variational autoencoders VAE follows from variational bayes inference.
	They aer generative models that attempt to descrive data generation through a probabilistic distribution.
	Given an observed dataset $X=\{x_i\}^N_{i=1}$ of $V$ samples, a generative model for each datum $x_i$ conditioned on an unobserved random latent variable $z_i$ is assumed, where $\theta$ are the parameters governing the distribution.
	This is equivalent to a probabilistic decoder.
	Symmetrically an approximate posterior distribution is assumed over $z_i$ given $x_i$, a probabilistic decoder governed by $\phi$.
	A prior distribution is assumed for $z_i$: $p_\theta(z_i)$.
	$\theta$ and $\phi$ need to be learned from data.
	$z_i$ can be interpreted as a code given by the recognition model $q_\phi(z|x)$/
	The marginal log-likelihood is expressed as sum over the individual data points $\log p_\theta(x_1, \dots, x_N) = \sum\limits_{i=1}^N\log p_\theta(x_i)$ and each point can be rewritten:

	$$\log p_\theta(x_i) = D_{KL}(q_\phi(z|x_i) || p_\theta(z|x_i)) + \mathcal{L}(\theta, \phi, x_i)$$

	Where the first term is the Kullback-leibler divergence of the approximate recognition model from the true posterior and the second is the variational lower bound on the marginal likelihood:

	$$\mathcal{L}(\theta, \phi, x_i) = \mathbb{E}_{q_\phi(z|x_i)}[-\log q_\phi(z|x) + \log p_\theta(x, z)]$$

	Maximizing this lower bound improves approximation of the posterior and can be expanded as:

	$$\mathcal{L}(\theta, \phi, x_i) = -D_{KL}(q_\phi(z|x_i)||p_\theta(z)) + \mathbb{E}_{q_\phi(z|x_i)}[\log p_\theta(x_i|z)]$$

	Variational inference follows by maximizing this lower bound for all data points with respect to $\theta$ and $\phi$.
	Given a dataset $X=\{x_i\}^N_{i=1}$ with $N$ data points, the marginal likelihood lower bound of the full dataset can be estimated using a mini-batch of size $M$:

	$$\mathcal{L}(\theta, \phi, x_i) \approx \tilde{\mathcal{L}}^M(\theta, \phi, X^M) = \frac{N}{M}\sum\limits_{i=1}^M\mathcal{L}(\theta, \phi, x_i)$$

	With its gradients approximated using a reparameterization trick and stochastic gradient optimization.

		\subsubsection{The reparameterization trick}
		The random variable $\tilde{z}\sim q_\phi(z|x)$ can be reparameterized using a differentiable transformation $g_\phi(\epsilon, x)$ using auxiliary noise $\epsilon$ drawn from some distribution:

		$$\mathcal{L}(\theta, \phi, x_i) \approx \tilde{\mathcal{L}}(\theta, \phi, x_i) = \frac{1}{L}\sum\limits_{l=1}^L\log p_\theta(x_i, z_{(i, l)}) - \log q_\phi(z_{(i, l)}|x_i)$$

		Where $z_{(i, l)} = g_\phi(\epsilon_{(i, l)}, x_i)$ with $\epsilon_{(i, l)}$ is random noise.
		To optimized mini-batch estimates we use the differentiable with respect to $\theta$ and $\phi$ equation:

		$$\hat{\mathcal{L}}^M(\theta, \phi, X) = \frac{N}{M}\sum\limits_{i=1}^M\tilde{\mathcal{L}}(\theta, \phi, x_i)$$

		$L$ can be set to $1$ if $M$ is large enough.
		Instead of taking the gradient of a single randomized latent representation, the gradients of the generative network are learned by a weighted average of the sample over different samples from its posterior distribution.
		The weights follows the likelihood functions $q_\phi(z_{(j, l)}|x_i)$.

		\subsubsection{Disentangled autoencoders}
		The variational lower bound can be viewed as the summation of the reconstruction capability of the samples and the regularization that biases $q_\phi(z|x^{(i)})$ towards the assumed prior $p_\theta(z)$.
		Disentangled autoencoders introduce another parameter $\beta$ that is a multiplicative factor for the KL divergence:

		$$\mathcal{L}(\theta, \phi, x_i) = -\beta D_{KL}(q_\phi(z|x_i)||p_\theta(z)) + \mathbb{E}_{q_\phi(z|x_i)}[\log p_\theta(x_i|z)]$$

		The prior $p_\theta(z)$ is set as the standard multivariate normal distribution.
		All the features are uncorrelated and the KL divergence regularize the latent features distribution $q_\phi(z|x^{(i)}$ to a less correlated one.
		The larger the $\beta$ the less correlated the features will be.

	\subsection{Applications of autoencoders}

		\subsubsection{Generative model}
		Once the autoencoder is trained, to obtain new data from it one can simply sample random variables from the same prior $p_\theta(z)$ and feed it to the decoder, which would generate a new meaningful sample.

		\subsubsection{Classification}
		Once the autoencoder is trained in an unsupervised manner one can use its decoder to do feature extraction for classification.
		The key assumption is that samples with the same label should correspond to some latent presentation.
		Another approach is to use them as a regularization technique.

		\subsubsection{Clustering}
		Once the autoencoder is trained in an unsupervised manner one can use its decoder to obtain a low-dimensional representation used to solve the dimensionality problem of clustering.
		Also during clustering the embeddings aer retrained at each iteration, adding an argument to the autoencoder loss function, penalizing the distance between the embedding and the cluster centre.

		\subsubsection{Anomaly detection}
		A trained autoencoder would learn the latent subspace of normal samples.
		Once trained it would result with a low reconstruction error for normal samples and high for anomalies.

		\subsubsection{Dimensionality reduction}
		The dimensionality reduction is performed by every autoencoder in the bottleneck layer.
		The non-liearity activation functions often allows for a superior reconstruction when compared to simple PCA.

	\subsection{Advanced autoencoder techniques}
	Autoencoders' strength is their ability to use their latent representation for different usages, but their reconstructions are usually blurry.
	The reason for that is the used loss function.

		\subsubsection{Autoencoders and GANs}
		GANs create results visually compelling but in the cost of the control on the resulting images.
		So a work has been done to combine autoencoders and GANs.
		In adversarial autoencoders the KL-divergence in VAE is replaced by a discriminator network that distinguishes between the prior and the approximated posterior.
		It can be done replacing the reconstruction loss or making the encoder and the discriminator share weights.

			\paragraph{Adversarially learned inference}
			In adversarially learned inference ALI there is an attempt to merge VAEs and GANs.
			A discriminator is used to distinguish between $(x, \hat{z})$ pairs, where $x$ is an input and $z\sim q(z|x)$ is sampled from the encoders output and $(\tilde{x}, z)$ pairs, where $z\sim p(z)$ sampled from the prior in VAE and $\tilde{x}\sim p(x|z)$ is the decoder's output.

			\paragraph{Wasserstein autoencoders}
			Wasserstein-GAN WGAN solve a lot of problems to the learning stability of GANs using the Wasserstein distance for the optimizations loss function.
			It is the distance between two probabilities:

			$$W_c(P_X, P_G) = \inf\limits_{\Gamma\in P(X\sim P_x, Y\sim P_G)}\mathbb{E}_{(X, Y)\sim\Gamma}[c(X, Y)]$$

			Where $c(x, y)$ is some cost function.
			When $c(x,y) = d^p(x, y)$ is a metric measurement, the $p$-th root of $W_c$ is called the p-Wasserstein distance.
			When $c(x, y) = d(x, y)$ acn be defined as:

			$$W_1(P_X, P_G) = \sup\limits_{f\in\mathcal{F}} \mathbb{E}_{X\sim P_X}[f(X)] - \mathbb{E}_{Y\sim P_G}[f(Y)]$$

			IN Wasserstein autoencoders the loss function is modified and the objective becomes:

			$$D_{WAE}(P_X, P_G) = \inf\limits_{Q(Z|X)\in\mathcal{Q}}\mathbb{E}_{P_X}\mathbb{E}_{Q(Z|X)}[c(X, G(Z))] + \lambda\cdot D_Z(Q_Z, P_Z)$$

			Where $Q$ is is the encoder and $G$ is the decoder.
			The left part is the new reconstruction loss, which penalizes on the output distribution and the sample distribution.

		\subsubsection{Deep feature conssitenf variational autoencoder}
		Given an original image and a reconstructed one, a different measure is used that takes into account the correlation between pixels.
		A pretrained network can be used for creating a loss function for autoencoders.
		After encoding and decoding an image, both are inserted as input to a pretrained network.
		Assuming results with high accuracy, and a not so different domain, eac layer can be seen as a successful feature extractor of the input image.
		Therefore instead of measuring the difference between two images, it can be measured between their representation, imposing a more realistic different measure for the autoencoder.

\section{Performance comparison of deep learning autoencoders for cancer subtype detection using multi-omics data}

	\subsection{Introduction}
	Different omics technologies provide the amility to interpret and explain the genome through DNA sequencing, genome expression, protein identification and others.
	Such individual data is limited in the inofrmation on the molecular complexity occurring inside the organism.
	Furthermore, the dimension and diversity of such data make it extremely challenging to perform proper data analysis that can efficiently fuse these data.
	Multi-omics data integration is the method in which diverse types of omics data are combined as predictor variables.
	This methods can allow for the identification of crucial genomic factors and biomarker, generate models and understand the genetics and genomics architecture of complex phenotypes.
	Several data fusion model have been proposed: early, intermediate and late fusion.
	One example is similarity network fusion where diverse types of data are normalized into a network through a non linear kernel function, next these networks are fused through an iterative fusion algorithm.
	Also the deep-learning framework of autoencoders exhibited significant potential as data fusion algorithm.
	The autoencoder generates new non linear features from its original input feature set.
	They are algorithms for dimensionality reduction and heterogeneous data integration based on feed-forward neural networks.
	They have a butterfly structure with number of neuron in input equals to the one in output and smaller hidden layers.
	This design creates a compressed representation of the data while preserving the input data's most important feature.
	It also allows it to concatenate the features and information of different omics sources.
	A critical application of such data fusion algorithms is cancer subtype detection using omics data.
	THe performance of four different autoencoders to integrate and reduce multi-omics data is used in this work.
	By data fusion the autencoders created new features to represent the input datasets., used to implement a survival-based clustering algorithm to define groups of patients with a similar distribution of features and survival prognosis.

	\subsection{Materials and methods}

		\subsubsection{Dataset and prepossessing}
		From the TGCA database data from glioblastoma multiforme, colon adenocarcinoma, kidney renal clear cell carcinoma and breast invasive carcinoma with data of gene expression, DNA methylation and miRNA expression.
		From the datasets the most common patients where chosen and each data was scaled according to $x_n = \frac{x_i-x_{min}}{x_{max} - x_{min}}$.
		The most important features where selected based on maximum variance.
		These selected features where fed into the autoencoders as input.

		\subsubsection{Autoencoder construction}

			\paragraph{Vanilla autoencoder}
			A vanilla autoencoder minimizes:

			$$L(x, g(f(x))$$

			Due to the non linearity of the encoder and decoder's activation function, the vanilla encoder learns non linear features from the data.
			A vanilla autoencoders with multiple hidden layers is called a deep vanilla autoencoder.
			This autoencoder has a high possibility of over-fitting.

			\paragraph{Denoising autoencoder}
			A denoising autencoder reconstructs the original input from a corrupt copy of an input minimizing:

			$$L(x, g(f(\tilde{x}))$$

			THe corrupt copy is formed introducing noise to the original input and denoising is achieved through stochastic mapping some input values to zero.
			This helps the autoencoder learn features other than the original features directly from the data.

			\paragraph{Sparse autoencoder}
			A parse autoencoder is a regularized version of vanilla autoencoder with a sparsity penalty $\Omega(h)$ added to the bottleneck layer.
			The learning minimizes:

			$$L(x, g(f(x))+\Omega(h)$$

			This helps to learn the important features of data even when there are meany hidden units in the autoencoder.

			\paragraph{Variational autoencoder}
			A variational autoencoder uses a strung assumption about latent variables, generally using a latent gaussian distribution.
			It imposes a constraint in the encoder network, forcing the bottleneck layer to follow a Gaussian distibution.
			It minimizes:

			$$L(x, g(f(x))+L(l)$$

			Where $L(l)$ is the latend loss, measured in terms of the Kullback-Leibler divergence of the bottleneck layer to a unit gaussian distribution, quantifying the difference between them.
			THis generates the latent variable with a generalization of the network.

		\subsubsection{Implementation}
		For vanilla, denoising and sparse autoencoders, the hidden layers where $500$, $100$ and $500$ nodes and input and output of $1000$.
		They were selected based on the maximum variance of three data types.
		For the denoising a noise factor of $0.5$ was applied.
		For sparse a $L_1$ regularization penalty of $0.01$ and an $L_2$ regularization penalty of $0.02$ on the nodes to induce sparsity was implemented.
		Log variance and lambda layer were used to convert the standard deviation for numerical stability when necessary.
		To optimize the autoencoders the ADAM algorithm was used and for vanilla, sparse and denoising the activation function for the input and hidden layer was the hyperbolic tangent and for the output the sigmoid.
		For the variational a ReLU on the input and hidden and sigmoid on the output was used.
		Tho measure the loss vbetween the input and output the means square error function was used for vanilla and denoising, the binary cross-entropy for sparse and the negative log-likelihood for variational.

		\subsubsection{Clustering and subtyping}
		The reduced number of features obtained from the bottleneck layer was used to apply standard subtyping methods.
		First the similarity of each patient pair was calculated by euclidian distance and spearman correlation.
		Then a clustering algorithm was used to cluster similar groups.
		An unsupervised subtypes discovery method combined with k-means and partitioning around medoids was used for clustering.
		k-means and PAM were used in a window between $3$ and $6$ clusters.

		\subsubsection{Evaluation metrics for subtyping}
		To evaluate the performance of the autoencoders survival analysis was used to evaluate the survival patterns from different subtypes.
		Next the $p$-value of the log-rank test was used to identify the difference in Kaplan-Meier survival curves between different subtypes.
		Low $p$-value ensure high confidence of different survival times for the different identified subtypes.
		Also the silhouette width of the clusters was used to benchamrk the performance of clustering.
		Silhouette scores measure how well a patient is matched to its identified cluster compared to other clusters.
		High silhouette scores indicates a proper group distribution.

		\subsubsection{COX model for feature selection}
		To validate data fusion, the two datasets that obtained the lowest results with the feature selection by variance were selected and COX was used to make a new selection.
		These new selected feature were used as input for the four different autoencoder implementations.

		\subsubsection{Comparison with other data integration methods}
		The results obtained were compared with SNF, PCA, kernel PCA and sparse PCA.
		SNF fuses the similarty network to aggregate multi-omics data.
		COX was used for the feature selection.
		After that cluster from $3$ to $6$ where used for the clustering.
		PCA, kernel PCA (non-linear), and sparse PCA (regularized) where used to transform datasets in which features where selected based on variance as input to k-means/PAM clutering.

		\subsubsection{Differential expression and enrichment analysis on detected subtypes}
		A differential expression and functional enrichment analysis of the clusters was perfumed.
		DE genes and enriched processes where detected using LIMMA and ClusterProfiler respectively.


	\subsection{Results}

		\subsubsection{Performance of different autoencoders}
		The silhouette score differs depending upon the regularization method: the optimal cluster number where selected counting the number of autoencoders that achieved a high silhouette score.
		A log-rank test were used to check if the identified clusters have different survival profiles.
		The lowest p-values with a high silhouette score for the optimal cluster number were considered as the final cluster prediction.
		There is no single winner architecture and the performance varies depending on the dataset.

		\begin{itemize}
			\item[GBM] Cluster number $3$, variational autoencoder with PAM/Spearman.
			\item[Coad] Cluster number $3$, vanilla autoencoder with PAM-Spearman.
		\end{itemize}

		\subsubsection{Different similarity measures}
		PAM clustering with Spearman performed favorably than k-means clustering with Euclidian distance on the silhouette score, but worse on p-value for the survival difference between the identified clusters.

		\subsubsection{Supervised feature selection}
		Using a COX model there was a significant improvement of the $p$ value for survival difference between the clusters, but a decrese in the silhouette score.
		Based on silhouette score VAE with Spearman was best.

		\subsubsection{Other subtype detection methods}
		PCA performed poorly, SNF comparable performance to autoencoders, but it has a few additional hyperparameters and the result is sensitive to their selection.

\section{Variational autoencoders for cancer data integration: design principles and computational practice}

	\subsection{Introduction}
	A complex biologica process is a combination of many simpler processes and its function is greater than the sum of its parts.
	Integrating and analysing simultaneously different data types offers better understanding of the mechanism.
	VAEs generate meaningful latent representations of integrated data that can be used for analysis bt a machine learning technique or be deployed on other heterogeneous datasets.
	The different machine learning approaches that integrate diverse data can be classified in:

	\begin{itemize}
		\item Output or late integration: different data are modelled separately and the output is the combined.
		\item Partial or intermediate integration: they produce a joint model learned from multiple data simultaneously.
		\item Full or early integration: different data is combined before applying a learning algorithm by aggregating them or by learning a common latent representation.
	\end{itemize}

	Unsupervised learning approaches learn representations by identifying patterns in the data and extracting meaningful knowledge while overcoming data complexities.
	Autoencoders are variants of deep learning networks that have good performance for unsupervised representation learning.
	The compressed representation they learn should capture the structure of the data and allow for more accurate downstream analysis.
	Now variational bayesian inference autoencoders learn the parameters of the underlying distribution of the input data and be utilized as methods for full integration of data.
	In this way they allow learning representations from heterogeneous data on different scales from different sources.

	\subsection{Materials and methods}
	VAEs are combined into a number of different architectures for a deep analysis and comparison with respect to specific data features and tasks.
	They are suitable since they are generative, non-linear, unsupervised and amendable to integrating diverse data.

		\subsubsection{Variational autoencoders}
		An autoencoder consist of an encoder that maps the high dimensional input data into a latent variable embedding with lower dimension and a decoder that attempts to reconstruct the input data from the embedding.
		There is a decoder function $f$ and an encoder one $g$.
		The lower dimensional embedding is $h = g_\phi(x)$ and the reconstructed input $x'=f_\theta(g_\phi(x))$.
		$\theta$ and $\phi$ are learned together to output a reconstructed data sample ideally the same as the original input.
		To quantify the error one can use the cross entropy or the mean squared error.
		The main variants to autoencoders aim to address improving generalization, disentanglement and modification to sequence input models.
		The VAE uses stochastic inference to approximate the latent variables $z$ as probability distributions.
		This represents and capture relevant features from the input.
		The are scalable and can deal with intractable posterior distributions by fitting an approximate inference or recognition model using a reparameterized variational lower bound estimator.
		The high dimensional data $x$ is drawn from a random variable with distribution $p_{data}(x)$.
		It asssumes that $x$ lies in a lower dimensional space that can be characterized by an unobserved continuous random variable $z$.
		The prior $p_\theta(z)$ and likelihood $p_\theta(x|z)$ come from a family of parametric distributions, with its probability density functions differentiable almost everywhere.
		$\theta$ and the values of $z$ are unknown and VAE approximates $p_\theta(x|z)$ using a recognition model $q_\theta(z|x)$ and the learned parameter $\phi$.
		A VAE build an inference where given a data-point $x$ it produces a distribution over the latent values $z$.
		A probabilistic decoder will, given a certain value of $z$, produce a distribution over the possible corresponding values of $x$, constructing the likelihood $p_\theta(x|z)$.
		Latent variables are tipically assumed centred isotropic multivariate gaussian $p_\phi(z) - N(z;0, I)$ and $p_\theta(x|z)$ a multivariate Gaussian or bernoulli with parameters approximated using a fully connected NN.
		The posterior is intractable and it is assumed it takes the form of a gaussian with an approximately diagonal covariance.
		This allows to approximate the true posterior, converting the problem in an optimization one.
		The variational approximate posterior will be:

		$$q_\phi(z|x^{(i)}) = N(z, \mu^{(i)}, \sigma^{(i)} I)$$

		Where the mean and the variance are outputs of the encoder.
		The discrepancy between $p_\theta(z)$ and $q_\phi(z|x^{(i)})$ can be directly computed and differentiated.
		The likelihood will be:

		$$l_i(\theta, \phi) = -\mathbb{E}_{q_\phi(z|x^{(i)})}[\log p_\theta(x|z)] + KL(q)\phi(z|x^{(i)}||p_\theta(z))$$

		There are other choice for the prior other than the Gaussian like a mixture of Gaussians, or like a mixture of approximate posteriors, a Dirichlet process as a non-parametric prior through stick-breaking process, generalizing over the generative process, or a graphical model.
		Other posteriors are normalizing flows, auto-regressive flows and inverse auto-regressive flows.
		The influence of the disentanglement factore is controlled using a parameter $\beta$.
		Other retularizatio nterms can be maximum mean discrepancy MMD instead of KL diveregns.

		$$MMD(q(z)||p(z)) = \mathbb{E}_{p(z), p(z')}[k(z, z')] + \mathbb{E}_{q(z), q(z')}[k(z, z')] -2\mathbb{E}_{q(z), p(z')}[k(z, z')]$$

		Where $k(z, z')$ is any universal kernel.

		\subsubsection{Variational autoencoders for data integration}

			\paragraph{Variational autoencoder with concatenated inputs}
			In CNC-VAE the encoder is directly trained from different data sets aligned and concatenated at input.
			Beside this the rest of the architecture is a standard VAE: the input data is first scaled, aligned and concatenated before being fed to the network.
			It has one objective function that reconstructs the combined data.
			It aims at reducing redundancies and extracting meaningful structure across all input sources.

			\paragraph{X-shaped variational autoencoder}
			X-VAE merges high level representations of several heterogeneous data sources into a single latent representation by learning to reconstruct the input data from the common homogeneous representation.
			The architecture consists of individual branches, one for each data source that are combined into one before the bottleneck layer.
			In the decoding phase the merged ranch splits again into several branches that produce individual reconstructions of the inputs.
			It combines different loss functions for each data source, allowing for better learning an more meaningful repreentations.

			\paragraph{Mixed-modal variational autoencoder}
			MM-VAE attempts to address some of the limitations of X-VAE employing a more gradual integration in the hidden layers of the encoder.
			It builds upon transfer learning.
			It consists of branches that individually reconstruct the input data sources, however those share information with each other in the encoding phase.
			Higher-level concepts are shared between all and used deeper in the network.
			So the information from different sources can be combined more gradually.
			The objective function combines different reconstruction loss functions.

			\paragraph{Hiearachical variational autoencoder}
			H-VAE builds upon meta-learning approaches for combining multiple individual models.
			It is comprised of several low level VAE that relate to each data source and the result is assembled together in a high-level VAE.
			Each of the low-level VAEs in emplyed to elarn a representation of an individual data source that are then merged together and fed to another VAE that produces the integrated data representation.
			The input to the high-level autoencoder implement distribution regularization terms in each of them separately and consist of approximated multivariate standard normal distributions.

		\subsubsection{Experiment setup}
		The aim of the evaluation is to seek the optimal configuration of choosing the appropriate objective function and parameters.
		Then evaluate and chose the most appropriate architectures for the data integration task performing a comparative quantitative analysis of the representations obtained from each of the architectures.
		Finally the results are chosen in terms of their application to cancer data integration.
		The genes were selected with the most significant Bonferrroni adjusted p-value.
		After missing data removal the input data consisted of $1000$ fatures.
		All the datasets where sampled into five-fold cross-validation splits for each classification task.
		The depth of the architectures remained moderate and constant.
		All designs expect for MM-VAE the encoder and decoder were symmetric and consisted of compression and decompression dense layers placed before and after data merging.
		MM-VAI had an additional data-merging layer in the encoder network.
		They had depth between $2$ and $4$ hidden layers.
		They all used batch normalization and exponential linear unit activation.
		They had a hidden dropout with rate $0.2$
		The final layer employed sigmoid activation function.
		They used the Adam optimizer with $lr=0.001$ and batch size of $64$.
		To chose the optimal objective function that consider the reconstruction loss and a regularization term.
		For the former they used binary cross entropy loss and mean squared error loss.
		For the latter wighted KL and weighted MMD with different values of $\beta$.
		To evaluate the quality of the representations thwy used three integrative tasks, training a predictive model on the produced representation and measuring its predictive performance on a binary classification task of IHC cancer subtypes.
		For this they trained and evaluated a gaussian naive Bayes classifier.
		Then, they evaluated the performance of three different methods trained on different representation: Gaussian naive Bayes classifiaer, SVMs and Random Forest.
		These result where compared with the raw data and data transformed with PCA.
